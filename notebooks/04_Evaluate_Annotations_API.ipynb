{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96521a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from requests.compat import urljoin\n",
    "import requests\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e71ed1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.9\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d97b4a1c-2e0a-48fd-8400-e6e4cd9261ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/stirunag/work/github/otar-maintenance/notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0187d8c5-3418-492a-9917-5d7445ad1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('annotations_api/annotations_api_CD_GP_DS_OG_test.csv', sep = ',', names=['pmcid', 'section', 'sentence','ner_list', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e96b9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sub_span(token_span, entity_span):\n",
    "    if token_span[0] < entity_span[1] and token_span[1] > entity_span[0]:\n",
    "        return max(token_span[0], entity_span[0]), min(token_span[1], entity_span[1])\n",
    "    return None\n",
    "\n",
    "def convert2IOB(text_data, ner_tags):\n",
    "    tokens = wordpunct_tokenize(text_data)\n",
    "    # Compute the start and end positions for each token\n",
    "    current_pos = 0\n",
    "    token_spans = []\n",
    "    for token in tokens:\n",
    "        start = text_data.find(token, current_pos)\n",
    "        end = start + len(token)\n",
    "        token_spans.append((start, end))\n",
    "        current_pos = end\n",
    "\n",
    "    iob_tags = ['O'] * len(tokens)\n",
    "\n",
    "    for start, end, entity, entity_type in sorted(ner_tags, key=lambda x: x[0]):\n",
    "        entity_flag = False\n",
    "        for i, token_span in enumerate(token_spans):\n",
    "            if start <= token_span[0] < end:  # Adjusted condition for finding sub-span\n",
    "                if not entity_flag:\n",
    "                    iob_tags[i] = 'B-' + entity_type\n",
    "                    entity_flag = True\n",
    "                else:\n",
    "                    iob_tags[i] = 'I-' + entity_type\n",
    "            else:\n",
    "                entity_flag = False\n",
    "\n",
    "    # Validate tag sequence\n",
    "    for i in range(1, len(iob_tags)):\n",
    "        if iob_tags[i].startswith('I-') and iob_tags[i-1] == 'O':\n",
    "            print(f\"Invalid tag sequence at position {i} in sentence: {text_data}\")\n",
    "            # Optionally, handle the invalid sequence here\n",
    "\n",
    "    return list(zip(tokens, iob_tags))\n",
    "\n",
    "\n",
    "def convert2IOB_batch(docs, ner_tags_batch):\n",
    "    results = []\n",
    "\n",
    "    # Process the texts in batches using nlp.pipe\n",
    "    for doc, ner_tags in zip(docs, ner_tags_batch):\n",
    "        tokens = [token.text for token in doc]\n",
    "        token_spans = [(token.idx, token.idx + len(token.text)) for token in doc]\n",
    "        iob_tags = ['O'] * len(tokens)\n",
    "        for start, end, entity, entity_type in sorted(ner_tags, key=lambda x: x[0]):\n",
    "            entity_flag = False\n",
    "            for i, token_span in enumerate(token_spans):\n",
    "                if find_sub_span(token_span, (start, end)):\n",
    "                    if not entity_flag:\n",
    "                        iob_tags[i] = 'B-' + entity_type\n",
    "                        entity_flag = True\n",
    "                    elif iob_tags[i] == 'O':\n",
    "                        iob_tags[i] = 'I-' + entity_type\n",
    "                else:\n",
    "                    entity_flag = False\n",
    "\n",
    "        results.append(list(zip(tokens, iob_tags)))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8a6658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pathlib\n",
    "from nltk.tokenize import wordpunct_tokenize \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323686d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_IOB_format(data_file, output_folder, filename):\n",
    "    df = pd.read_csv(data_file)\n",
    "\n",
    "    # Convert 'ner' column string representation of list to actual list if it's not None\n",
    "    data = [(row['sentence'], eval(row['ner']) if isinstance(row['ner'], str) and row['ner'] != \"[]\" else []) for index, row in df.iterrows()]\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    pathlib.Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "    result_path = os.path.join(output_folder, filename)\n",
    "\n",
    "    with open(result_path, 'w', newline='\\n') as f1:\n",
    "        train_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "\n",
    "        # Process each sentence individually\n",
    "        for sentence, ner_tags in data:\n",
    "            iob_tags = convert2IOB(sentence, ner_tags)  # Convert to IOB format\n",
    "\n",
    "            # Write the IOB-tagged tokens to the file\n",
    "            for token, tag in iob_tags:\n",
    "                train_writer.writerow([token, tag])\n",
    "            train_writer.writerow([])  # Add an empty line after each sentence\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "662184ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IOB_file_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotations_api/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m data_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotations_api/annotations_api_CD_GP_DS_OG_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m convert_to_IOB_format(data_file, output_folder,\u001b[43mIOB_file_name\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'IOB_file_name' is not defined"
     ]
    }
   ],
   "source": [
    "output_folder = 'annotations_api/'\n",
    "data_file = 'annotations_api/annotations_api_CD_GP_DS_OG_test.csv'\n",
    "convert_to_IOB_format(data_file, output_folder,IOB_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d90801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from collections import namedtuple\n",
    "from typing import List, Tuple, Dict\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')\n",
    "\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(ch)\n",
    "\n",
    "Entity = namedtuple('Entity', ['span', 'tag'])\n",
    "Entity_Label = namedtuple('Label', ['index', 'pos', 'tag'])\n",
    "\n",
    "\n",
    "class MatchCount:\n",
    "    \"\"\"\n",
    "    class to record the count of matching\n",
    "    \"\"\"\n",
    "    def __init__(self, correct=0, incorrect=0, partial=0, missing=0, spurious=0):\n",
    "        self.correct = correct\n",
    "        self.incorrect = incorrect\n",
    "        self.partial = partial\n",
    "        self.missing = missing\n",
    "        self.spurious = spurious\n",
    "\n",
    "    def __eq__(self, other: 'MatchCount'):\n",
    "        \"\"\"\n",
    "        check the equality by attribute values\n",
    "        \"\"\"\n",
    "        if not isinstance(other, MatchCount):\n",
    "            return NotImplemented\n",
    "        isequal = self.correct == other.correct and self.incorrect == other.incorrect and \\\n",
    "            self.partial == other.partial and self.missing == other.missing and self.spurious == other.spurious\n",
    "        return isequal\n",
    "\n",
    "    def __add__(self, other: 'MatchCount'):\n",
    "        \"\"\"\n",
    "        add two matching counts\n",
    "        \"\"\"\n",
    "        if not isinstance(other, MatchCount):\n",
    "            return NotImplemented\n",
    "        correct = self.correct + other.correct\n",
    "        incorrect = self.incorrect + other.incorrect\n",
    "        partial = self.partial + other.partial\n",
    "        missing = self.missing + other.missing\n",
    "        spurious = self.spurious + other.spurious\n",
    "        return MatchCount(correct=correct, incorrect=incorrect, partial=partial, missing=missing, spurious=spurious)\n",
    "\n",
    "    def sum(self, gold: bool = True) -> int:\n",
    "        \"\"\"\n",
    "        sum the relevant counts together\n",
    "        :param gold: if the dataset is gold set\n",
    "        :type gold: bool\n",
    "        :return: ACT\n",
    "        :rtype:\n",
    "        \"\"\"\n",
    "        if gold:\n",
    "            return self.correct + self.incorrect + self.missing + self.partial\n",
    "        else:\n",
    "            return self.correct + self.incorrect + self.spurious + self.partial\n",
    "\n",
    "    def get_count(self) -> Tuple[int,int,int,int,int]:\n",
    "        return self.correct, self.incorrect, self.partial, self.missing, self.spurious\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"MatchCount: correct={self.correct}, incorrect={self.incorrect}, partial={self.partial}, \" \\\n",
    "            f\"missing={self.missing}, spurious={self.spurious}\"\n",
    "\n",
    "\n",
    "def has_overlap(gold: Entity, response: Entity) -> bool:\n",
    "    \"\"\"\n",
    "    check whether the gold entity and the response entity overlap\n",
    "    :param gold: gold entity\n",
    "    :type gold: Entity\n",
    "    :param response: response entity\n",
    "    :type response: Entity\n",
    "    :return: overlaps or not\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    return gold.span[0] < response.span[1] and response.span[0] < gold.span[1]\n",
    "\n",
    "\n",
    "def validate_label_seq(label_seq: List[str], ground_data: List[str] = None) -> bool:\n",
    "    \"\"\"\n",
    "    Validate the label sequence of a sentence\n",
    "    An invalid label sequence would be ['O', 'I-GP'] because an entity should start with 'B-'\n",
    "    :param label_seq: a list of actual label sequence of a sentence\n",
    "    :type label_seq: List[str]\n",
    "    :param ground_data: tokens of a sentence\n",
    "    :type ground_data: tokens of a sentence\n",
    "    :return: True for valid label sequence\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    # prohibited label sequence: 'O' -> 'I'\n",
    "    prev_pos, prev_tag = ('O', 'O')\n",
    "    for label in label_seq:\n",
    "        if label == 'O':\n",
    "            pos, tag = ('O', 'O')\n",
    "        else:\n",
    "            pos, tag = label.split('-')\n",
    "            if pos not in ('B', 'I'):\n",
    "                logger.warning(f'expect label starts with B or I but got: {label}')\n",
    "                return False\n",
    "        if pos == 'I':\n",
    "            if prev_pos == 'O':\n",
    "                logger.warning(f'invalid label sequence, I comes after O: {list(zip(label_seq, ground_data)) if ground_data else label_seq}')\n",
    "                return False\n",
    "            if prev_pos in ('B', 'I') and prev_tag != tag:\n",
    "                logger.warning(f'invalid label sequence, tag inconsistent: {list(zip(label_seq, ground_data)) if ground_data else label_seq}')\n",
    "                return False\n",
    "        prev_pos, prev_tag = pos, tag\n",
    "    return True\n",
    "\n",
    "\n",
    "def extract_entity(seq: List[str], *args) -> List[Entity]:\n",
    "    \"\"\"\n",
    "    extract entity from label sequence\n",
    "    :param seq: a list of labels in a sentence\n",
    "    :type seq: List[str\n",
    "    :return: A list of entity object\n",
    "    :rtype: List[Entity]\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    tmp = []\n",
    "\n",
    "    for i, token in enumerate(seq):\n",
    "        if token == 'O':\n",
    "            pos, tag = 'O', 'O'\n",
    "        else:\n",
    "            pos, tag = token.split('-')\n",
    "        label = Entity_Label(index=i, pos=pos, tag=tag)\n",
    "\n",
    "        if pos == 'B' or pos == 'O':\n",
    "            if tmp:\n",
    "                entities.append(Entity(span=(tmp[0].index, i), tag=tmp[0].tag))\n",
    "                tmp[:] = []\n",
    "            if pos == 'B':\n",
    "                tmp.append(label)\n",
    "        elif pos == 'I':\n",
    "            tmp.append(label)\n",
    "    if tmp:\n",
    "        entities.append(Entity(span=(tmp[0].index, tmp[-1].index+1), tag=tmp[0].tag))\n",
    "    return entities\n",
    "\n",
    "\n",
    "def agreement_sentence(entities_gold: List[Entity], entities_response: List[Entity]) -> \\\n",
    "        Tuple[MatchCount, MatchCount, MatchCount, MatchCount]:\n",
    "    \"\"\"\n",
    "    calculate the correct, partial, missing and spurious entities between gold and response entities\n",
    "    :param entities_gold: A list of entities from gold labels of a sentence\n",
    "    :type entities_gold: List[Entity]\n",
    "    :param entities_response: A list of entities from response labels of a sentence\n",
    "    :type entities_response: List[Entity]\n",
    "    :return: A tuple of match counts: [correct_total, partial_total, missing_total, incorrect_total, spurious_total]\n",
    "    :rtype: Tuple[int, int, int, int, int]\n",
    "    \"\"\"\n",
    "    strict = MatchCount()\n",
    "    exact_boundary = MatchCount()\n",
    "    partial_boundary = MatchCount()\n",
    "    type_matching = MatchCount()\n",
    "\n",
    "    missing_count_gold = [True for _ in range(len(entities_gold))]\n",
    "    spurious_count_resp = [True for _ in range(len(entities_response))]\n",
    "\n",
    "    for i, annoA in enumerate(entities_gold):\n",
    "        for j, annoB in enumerate(entities_response):\n",
    "            if has_overlap(annoA, annoB):\n",
    "                missing_count_gold[i] = False\n",
    "                spurious_count_resp[j] = False\n",
    "\n",
    "                # type matching: check overlap and type\n",
    "                if annoA.tag == annoB.tag:\n",
    "                    type_matching.correct += 1\n",
    "                else:\n",
    "                    type_matching.incorrect += 1\n",
    "\n",
    "                if annoA.span == annoB.span:\n",
    "                    # boundary match: check exact boundary\n",
    "                    exact_boundary.correct += 1\n",
    "                    partial_boundary.correct += 1\n",
    "                    # strict match: boundary ok and check type\n",
    "                    if annoA.tag == annoB.tag:\n",
    "                        strict.correct += 1\n",
    "                    else:\n",
    "                        strict.incorrect += 1\n",
    "                else:\n",
    "                    # strict: boundary not exact match\n",
    "                    strict.incorrect += 1\n",
    "                    # exact boundary match: boundary not exact match\n",
    "                    exact_boundary.incorrect += 1\n",
    "                    # partial match: partial match\n",
    "                    partial_boundary.partial += 1\n",
    "\n",
    "    missing = sum(missing_count_gold)\n",
    "    spurious = sum(spurious_count_resp)\n",
    "    # print(spurious_count_resp)\n",
    "\n",
    "    strict.missing, exact_boundary.missing, partial_boundary.missing, type_matching.missing = [missing]*4\n",
    "    strict.spurious, exact_boundary.spurious, partial_boundary.spurious, type_matching.spurious = [spurious]*4\n",
    "\n",
    "    return strict, exact_boundary, partial_boundary, type_matching\n",
    "\n",
    "\n",
    "def agreement_dataset(gold: List[List[str]], response: List[List[str]],\n",
    "                      validate_label: bool = True, X: List[List[str]] = None) -> Tuple[List[MatchCount], int, int]:\n",
    "    \"\"\"\n",
    "    clacuate the agreement between gold set and response set\n",
    "    :param gold: A list of label sequence of sentences from gold set\n",
    "    :type gold: List[List[str]]\n",
    "    :param response: A list of label sequence of sentences from response set\n",
    "    :type response: List[List[str]]\n",
    "    :param validate_label: if True, validate the label sequence\n",
    "    :type validate_label: bool\n",
    "    :param x: A list of text sequences\n",
    "    :type X: List[List[str]]\n",
    "    :return: A tuple of match counts and total counts:\n",
    "    [correct_total, partial_total, missing_total, incorrect_total, spurious_total], gold_total, resp_total\n",
    "    :rtype: Tuple[List[MatchCount], int, int]\n",
    "    \"\"\"\n",
    "    assert len(gold) == len(response), f\"expect {len(gold)} sentences but got {len(response)} sentences \" \\\n",
    "        f\"in response dataset\"\n",
    "    dataset_agreement = [MatchCount()]*4\n",
    "    gold_ent_count = 0\n",
    "    resp_ent_count = 0\n",
    "    for i, label_seq_gold in enumerate(gold):\n",
    "        if validate_label:\n",
    "            if X:\n",
    "                ground_data = X[i]\n",
    "            else:\n",
    "                ground_data = None\n",
    "            assert validate_label_seq(label_seq_gold, ground_data), f\"sent {i}: invalid gold label sequence\\n\"\n",
    "            # assert validate_label_seq(response[i], ground_data), f\"sent {i}: invalid response label sequence\\n\"\n",
    "            validate_label_seq(response[i], ground_data)\n",
    "\n",
    "        entities_gold = extract_entity(label_seq_gold, i, 'gold')\n",
    "        entities_resp = extract_entity(response[i], i, 'response')\n",
    "\n",
    "        gold_ent_count += len(entities_gold)\n",
    "        resp_ent_count += len(entities_resp)\n",
    "\n",
    "        sent_agreement = agreement_sentence(entities_gold, entities_resp)\n",
    "        dataset_agreement = list(map(add, dataset_agreement, sent_agreement))\n",
    "    return dataset_agreement, gold_ent_count, resp_ent_count\n",
    "\n",
    "\n",
    "def evaluate(agreement: MatchCount, beta: float = 1.0) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    calculate the precision, recall and f1 score based on agreement\n",
    "    :param agreement: A tuple of match counts:\n",
    "    [correct_total, partial_total, missing_total, incorrect_total, spurious_total]\n",
    "    :type agreement: Tuple[int, int, int, int, int]\n",
    "    :param beta: beta is chosen such that recall is considered beta times as important as precision\n",
    "    :type beta: float\n",
    "    :return: precision, recall, f1 score\n",
    "    :rtype: Tuple[float, float, float]\n",
    "    \"\"\"\n",
    "    p = precision_score(correct=agreement.correct,\n",
    "                        incorrect=agreement.incorrect,\n",
    "                        partial=agreement.partial,\n",
    "                        spurious=agreement.spurious)\n",
    "    r = recall_score(correct=agreement.correct,\n",
    "                     incorrect=agreement.incorrect,\n",
    "                     partial=agreement.partial,\n",
    "                     missing=agreement.missing)\n",
    "    f1 = f1_score(precision=p, recall=r, beta=beta)\n",
    "    return p, r, f1\n",
    "\n",
    "\n",
    "def semeval_scores(gold: List[List[str]], response: List[List[str]], digits: int = 3,\n",
    "                   validate_label: bool = True, X: List[List[str]] = None) -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    calculate the precision, recall and f1 score based on agreement\n",
    "    :param gold: A list of label sequence of sentences from gold set\n",
    "    :type gold: List[List[str]]\n",
    "    :param response: A list of label sequence of sentences from response set\n",
    "    :type response: List[List[str]]\n",
    "    :param digits: Number of digits for formatting output floating point values\n",
    "    :type digits: int\n",
    "    :param validate_label: if True, validate the label sequence\n",
    "    :type validate_label: bool\n",
    "    :param X: A list of text sequences\n",
    "    :type X: List[List[str]]\n",
    "    :return: precision, recall, f1 score\n",
    "    :rtype: Tuple[float, float, float]\n",
    "    \"\"\"\n",
    "    match_counts = agreement_dataset(gold=gold, response=response, validate_label=validate_label, X=X)\n",
    "    scores = {}\n",
    "    for name, agree in zip(('strict', 'exact', 'partial', 'type'), match_counts):\n",
    "        p, r, f1 = evaluate(agree)\n",
    "        scores[name] = [round(p, digits), round(r, digits), round(f1, digits)]\n",
    "    return scores\n",
    "\n",
    "\n",
    "def semeval_scores_report(gold: List[List[str]], response: List[List[str]], digits: int = 2,\n",
    "                          validate_label: bool = True, X: List[List[str]] = None) -> str:\n",
    "    \"\"\"\n",
    "    calculate the precision, recall and f1 score based on agreement and generate a report\n",
    "    :param gold: A list of label sequence of sentences from gold set\n",
    "    :type gold: List[List[str]]\n",
    "    :param response: A list of label sequence of sentences from response set\n",
    "    :type response: List[List[str]]\n",
    "    :param digits: Number of digits for formatting output floating point values\n",
    "    :type digits: int\n",
    "    :param validate_label: if True, validate the label sequence\n",
    "    :type validate_label: bool\n",
    "    :param X: A list of text sequences\n",
    "    :type X: List[List[str]]\n",
    "    :return: formatted report\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    match_counts, gold_ent_count, resp_ent_count = agreement_dataset(gold=gold, response=response,\n",
    "                                                                     validate_label=validate_label, X=X)\n",
    "    HEADERS = ('strict', 'exact', 'partial', 'type')\n",
    "    SEMEVAL_NAMES = ('correct', 'incorrect', 'partial', 'missing', 'spurious')\n",
    "\n",
    "    strict_counts, exact_counts, partial_counts, type_counts = (count.get_count() for count in match_counts)\n",
    "    strict_scores, exact_scores, partial_scores, type_scores = (evaluate(agreement) for agreement in match_counts)\n",
    "\n",
    "    rows = zip(SEMEVAL_NAMES, strict_counts, exact_counts, partial_counts, type_counts)\n",
    "    longest_last_line_heading = 'Gold Total'\n",
    "    name_width = max(len(cn) for cn in SEMEVAL_NAMES)\n",
    "    width = max(name_width, len(longest_last_line_heading), digits)\n",
    "    head_fmt = '{:>{width}s} ' + ' {:>9}' * len(HEADERS)\n",
    "    report = head_fmt.format('', *HEADERS, width=width)\n",
    "    report += '\\n\\n'\n",
    "    row_fmt_score = '{:>{width}s} ' + ' {:>9.{digits}f}' * 4 + '\\n'\n",
    "    row_fmt_count = '{:>{width}s} ' + ' {:>9,d}' * 4 + '\\n'\n",
    "    row_fmt_ent = '{:>{width}s} ' + ' {:>9,d}' + '\\n'\n",
    "\n",
    "    row_str_len = 0\n",
    "    for row in rows:\n",
    "        report += row_fmt_count.format(*row, width=width)\n",
    "        row_str_len = len(row_fmt_count.format(*row, width=width))\n",
    "    report += '='*row_str_len + '\\n'\n",
    "    rows = zip(('precision', 'recall', 'f1 score'), strict_scores, exact_scores, partial_scores, type_scores)\n",
    "    for row in rows:\n",
    "        report += row_fmt_score.format(*row, width=width, digits=digits)\n",
    "    report += '='*row_str_len + '\\n'\n",
    "    rows = (('Gold Total', gold_ent_count), ('Resp Total', resp_ent_count))\n",
    "    for row in rows:\n",
    "        report += row_fmt_ent.format(*row, width=width)\n",
    "    report += '\\n'\n",
    "    return report\n",
    "\n",
    "\n",
    "def precision_score(correct: int, incorrect: int, partial: int, spurious: int) -> float:\n",
    "    \"\"\"\n",
    "    the denominator is the total number of entities produced by the system\n",
    "    the equation used:\n",
    "    ACT = COR + INC + PAR + SPU = TP + FP\n",
    "    P = COR/ACT = TP/(TP+FP)\n",
    "    if partial match allowed:\n",
    "    P = (COR + 0.5*PAR)/ACT\n",
    "\n",
    "    :param correct: number of gold entities exactly matched by response\n",
    "    :type correct: int\n",
    "    :param incorrect: number of gold entities wrongly annotated (wrong type) by response\n",
    "    :type incorrect: int\n",
    "    :param partial: number of gold entities partially annotated (same type and overlaps) by response\n",
    "    :type partial: int\n",
    "    :param spurious: number of response entities not in the gold entities\n",
    "    :type spurious: int\n",
    "    :return: precision score\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    numerator = correct + 0.5*partial\n",
    "    # A easier way to calculate denominator is by counting number of entities in response\n",
    "    # thus we dont need to count missing, incorrect and spurious\n",
    "    denominator = correct + incorrect + partial + spurious\n",
    "    return numerator/denominator\n",
    "\n",
    "\n",
    "def recall_score(correct: int, incorrect: int, partial: int, missing: int) -> float:\n",
    "    \"\"\"\n",
    "    the denominator is the total number of entities in the gold data\n",
    "    the equation used:\n",
    "\n",
    "    POS = COR + INC + PAR + MIS = TP + FN\n",
    "    R = COR/POS = TP/(TP+FN)\n",
    "    if partial match allowed:\n",
    "    R = (COR + 0.5*PAR)/POS\n",
    "\n",
    "    :param correct: number of gold entities exactly matched by response\n",
    "    :type correct: int\n",
    "    :param incorrect: number of gold entities wrongly annotated (wrong type) by response\n",
    "    :type incorrect: int\n",
    "    :param partial: number of gold entities partially annotated (same type and overlaps) by response\n",
    "    :type partial: int\n",
    "    :param missing: number of gold entities not in the response entities\n",
    "    :type missing: int\n",
    "    :return: recall score\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    numerator = correct + 0.5*partial\n",
    "    # A easier way to calculate denominator is by counting number of entities in gold\n",
    "    # thus we dont need to count missing, incorrect and spurious\n",
    "    denominator = correct + incorrect + partial + missing\n",
    "    return numerator/denominator\n",
    "\n",
    "\n",
    "def f1_score(precision: float, recall: float, beta: float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    calculate f1 score given precision and recall score\n",
    "    the equation used (beta f1 score):\n",
    "\n",
    "    F1 = (1+ beta**2) * (precision*recall) / ((beta**2)*precision + recall)\n",
    "    where beta is chosen such taht recall is considered beta times as important as precision\n",
    "\n",
    "    if beta = 1:\n",
    "    F1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "    :param precision: precision score\n",
    "    :type precision: float\n",
    "    :param recall: recall score\n",
    "    :type recall: float\n",
    "    :param beta: beta is chosen such that recall is considered beta times as important as precision,\n",
    "    :type beta: float\n",
    "    :return: f1 score\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    numerator = (beta**2 + 1)*precision * recall\n",
    "    denominator = (beta**2)*precision + recall\n",
    "    return numerator/denominator\n",
    "\n",
    "\n",
    "def load_IOBdataset(data_path: str, targets: List[str] = None) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    \"\"\"\n",
    "    load the IOB dataset, which is in csv format\n",
    "    :param data_path: path to the csv file of IOB dataset\n",
    "    :type data_path: str\n",
    "    :param targets: a list of interest types\n",
    "    :type targets: List[str]\n",
    "    :return: list of labels of every sentence in dataset\n",
    "    :rtype: List[List[str\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    X_sent = []\n",
    "    y_sent = []\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        csv_reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in csv_reader:\n",
    "            if line:\n",
    "                token, tag = line[0], line[-1]\n",
    "                X_sent.append(token)\n",
    "                if targets:\n",
    "                    if tag.split('-')[-1] in set(targets):\n",
    "                        y_sent.append(tag)\n",
    "                    else:\n",
    "                        y_sent.append('O')\n",
    "                else:\n",
    "                    y_sent.append(tag)\n",
    "            else:\n",
    "                # we reach the end of a sentence\n",
    "                if len(X_sent) > 0:\n",
    "                    X.append(X_sent)\n",
    "                    y.append(y_sent)\n",
    "                X_sent = []\n",
    "                y_sent = []\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def semeval_report(gold_path: str, response_path: str, targets: List[str] = None,\n",
    "                   digits: int = 2, validate_label: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    report semeval scores by providing file paths to gold and response datasets\n",
    "    :param gold_path: path to gold dataset file\n",
    "    :type gold_path: str\n",
    "    :param response_path: path to response dataset file\n",
    "    :type response_path: str\n",
    "    :param targets: a list of interest types\n",
    "    :type targets: List[str]\n",
    "    :param digits: Number of digits for formatting output floating point values\n",
    "    :type digits: int\n",
    "    :param validate_label: if True, validate the label sequence\n",
    "    :type validate_label: bool\n",
    "    :return: semeval scores report\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    gold_data, gold_labels = load_IOBdataset(gold_path, targets=targets)\n",
    "    _, resp_labels = load_IOBdataset(response_path, targets=targets)\n",
    "\n",
    "    if validate_label:\n",
    "        X = gold_data\n",
    "    else:\n",
    "        X = None\n",
    "    return semeval_scores_report(gold=gold_labels, response=resp_labels, digits=digits,\n",
    "                                 validate_label=validate_label, X=X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91451ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = output_folder+IOB_file_name\n",
    "all_tags = ['GP', 'DS', 'OG', 'CD']\n",
    "ml_path = output_folder+'test.tsv'\n",
    "\n",
    "\n",
    "for each_tag in all_tags:\n",
    "    print('############ '+each_tag+' ####################')\n",
    "    print('\\n')\n",
    "    print(semeval_report(gold_path=data_path, response_path=ml_path, targets=[each_tag]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5627dae-45b3-444a-95a4-f58f99f29060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for plotting\n",
    "categories = ['Precision', 'Recall', 'F1 Score']\n",
    "dictionary_chemical = [0.53, 0.34, 0.41]\n",
    "bioBERT_chemical = [0.91, 0.92, 0.92]\n",
    "spacy_chemical = [0.8, 0.73, 0.76]\n",
    "QEB8L_chemical = [0.85, 0.9, 0.88]\n",
    "\n",
    "dictionary_disease = [0.48, 0.74, 0.58]\n",
    "bioBERT_disease = [0.9, 0.8, 0.85]\n",
    "spacy_disease = [0.82, 0.71, 0.76]\n",
    "QEB8L_disease = [0.9, 0.88, 0.89]\n",
    "\n",
    "dictionary_organism = [0.68, 0.90, 0.78]\n",
    "bioBERT_organism = [0.93, 0.86, 0.9]\n",
    "spacy_organism = [0.85, 0.75, 0.79]\n",
    "QEB8L_organism = [0.94, 0.85, 0.89]\n",
    "\n",
    "dictionary_gene_protein = [0.48, 0.74, 0.58]\n",
    "bioBERT_gene_protein = [0.91, 0.87, 0.89]\n",
    "spacy_gene_protein = [0.84, 0.76, 0.8]\n",
    "QEB8L_gene_protein = [0.9, 0.88, 0.89]\n",
    "\n",
    "x = np.arange(len(categories))  # the label locations\n",
    "width = 0.15  # the width of the bars, adjusted to prevent overlap\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(14, 10), sharey=True)  # Share y-axis for consistency\n",
    "\n",
    "# Setup for increased font size to match scientific article standards\n",
    "plt.rcParams.update({'font.size': 14})  # Updating font size globally\n",
    "\n",
    "# Function to create bars for subplots\n",
    "def create_bars(ax, offset, data, label, color):\n",
    "    return ax.bar(x + offset, data, width, label=label, color=color)\n",
    "\n",
    "# Chemical/Drug subplot\n",
    "create_bars(ax[0, 0], -1.5*width, dictionary_chemical, 'Dictionary', 'cyan')\n",
    "create_bars(ax[0, 0], -0.5*width, bioBERT_chemical, 'BioBERT', 'skyblue')\n",
    "create_bars(ax[0, 0], 0.5*width, spacy_chemical, 'SpaCy', 'lightgreen')\n",
    "create_bars(ax[0, 0], 1.5*width, QEB8L_chemical, 'QEB8L', 'salmon')\n",
    "ax[0, 0].set_ylabel('Scores')\n",
    "ax[0, 0].set_title('Chemical/Drug')\n",
    "ax[0, 0].set_xticks(x)\n",
    "ax[0, 0].set_xticklabels(categories)\n",
    "\n",
    "# Disease subplot\n",
    "create_bars(ax[0, 1], -1.5*width, dictionary_disease, 'Dictionary', 'cyan')\n",
    "create_bars(ax[0, 1], -0.5*width, bioBERT_disease, 'BioBERT', 'skyblue')\n",
    "create_bars(ax[0, 1], 0.5*width, spacy_disease, 'SpaCy', 'lightgreen')\n",
    "create_bars(ax[0, 1], 1.5*width, QEB8L_disease, 'QEB8L', 'salmon')\n",
    "ax[0, 1].set_title('Disease')\n",
    "ax[0, 1].set_xticks(x)\n",
    "ax[0, 1].set_xticklabels(categories)\n",
    "\n",
    "# Organism subplot\n",
    "create_bars(ax[1, 0], -1.5*width, dictionary_organism, 'Dictionary', 'cyan')\n",
    "create_bars(ax[1, 0], -0.5*width, bioBERT_organism, 'BioBERT', 'skyblue')\n",
    "create_bars(ax[1, 0], 0.5*width, spacy_organism, 'SpaCy', 'lightgreen')\n",
    "create_bars(ax[1, 0], 1.5*width, QEB8L_organism, 'QEB8L', 'salmon')\n",
    "ax[1, 0].set_ylabel('Scores')\n",
    "ax[1, 0].set_title('Organism')\n",
    "ax[1, 0].set_xticks(x)\n",
    "ax[1, 0].set_xticklabels(categories)\n",
    "\n",
    "# Gene/Protein subplot\n",
    "create_bars(ax[1, 1], -1.5*width, dictionary_gene_protein, 'Dictionary', 'cyan')\n",
    "create_bars(ax[1, 1], -0.5*width, bioBERT_gene_protein, 'BioBERT', 'skyblue')\n",
    "create_bars(ax[1, 1], 0.5*width, spacy_gene_protein, 'SpaCy', 'lightgreen')\n",
    "create_bars(ax[1, 1], 1.5*width, QEB8L_gene_protein, 'QEB8L', 'salmon')\n",
    "ax[1, 1].set_title('Gene/Protein')\n",
    "ax[1, 1].set_xticks(x)\n",
    "ax[1, 1].set_xticklabels(categories)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust the rectangle to fit the legend\n",
    "\n",
    "# Add a single legend\n",
    "fig.legend(['Dictionary', 'BioBERT', 'SpaCy', 'QEB8L'], loc='upper center', ncol=4, bbox_to_anchor=(0.5, 1))\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig('annotations_api/comparison_plot.png', format='png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5de734-d767-4952-8b7f-e059eaaec835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvements\n",
    "QEB8L_scores = {\n",
    "    'chemical': [0.85, 0.9, 0.88],\n",
    "    'disease': [0.9, 0.88, 0.89],\n",
    "    'organism': [0.94, 0.85, 0.89],\n",
    "    'gene_protein': [0.9, 0.88, 0.89]\n",
    "}\n",
    "\n",
    "dictionary_scores = {\n",
    "    'chemical': [0.53, 0.34, 0.41],\n",
    "    'disease': [0.48, 0.74, 0.58],\n",
    "    'organism': [0.68, 0.90, 0.78],\n",
    "    'gene_protein': [0.48, 0.74, 0.58]\n",
    "}\n",
    "\n",
    "improvements = {category: [QEB8L - dictionary for QEB8L, dictionary in zip(QEB8L_scores[category], dictionary_scores[category])] \n",
    "                for category in QEB8L_scores}\n",
    "\n",
    "improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d0977-964f-4bef-9e07-857f1335600c",
   "metadata": {},
   "source": [
    "These results indicate that the quantized model (QEB8L) generally performs significantly better than the dictionary-based approach across most metrics and categories. The most notable improvements are seen in Precision and F1 Score across all categories, with substantial gains especially in Recall for the Chemical/Drug category. However, it's important to note the slight decrease in Recall for the Organism category, suggesting that the quantized model might prioritize precision over recall in some instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d2a56-da01-4c9c-b696-868bdbc91b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average scores for Precision (P), Recall (R), and F1 Score (F) across all categories for both QEB8L and Dictionary approaches\n",
    "average_scores_QEB8L = [sum(scores) / len(scores) for scores in zip(*QEB8L_scores.values())]\n",
    "average_scores_dictionary = [sum(scores) / len(scores) for scores in zip(*dictionary_scores.values())]\n",
    "\n",
    "average_scores = {\n",
    "    'QEB8L': average_scores_QEB8L,\n",
    "    'Dictionary': average_scores_dictionary\n",
    "}\n",
    "\n",
    "average_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59373d22-c009-4e69-9fb7-1698f2575149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
